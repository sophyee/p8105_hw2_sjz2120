---
title: "p8105_hw2_sjz2120"
output: github_document
date: "2022-09-28"
---

# Setup
```{r}
library(tidyverse)
library(readxl)
```


# Problem 1

## Data Import and Cleaning

Below we import and clean data from `NYC_Transit_Subway_Entrance_And_Exit_Data.csv`. First we use import the data using `read_csv()`, update variable names, and select the columns that will be used in later in this homework. We update the variable `entry` from `yes` / `no` to a logical variable. As part of the data import, we specify that `Route` columns 8-11 should be character for consistency with columns 1-7.

```{r}
subway_data =
  read_csv(file = "./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
           col_types = cols(Route8 = "c", Route9 = "c", Route10 = "c", Route11 = "c")) %>% 
  janitor::clean_names() %>% 
  select(line, station_name, station_latitude, station_longitude, starts_with("route"),
         entry, exit_only, vending, entrance_type, ada) %>% 
  mutate(entry = recode(entry, "YES" = TRUE, "NO" = FALSE ))
```

We still need to "tidy" up subway_data a bit. For instance, route number and route should be variables, so we need to convert all of the `route` variables from wide to long format. This will be useful when focusing on specific routes, but may not be necessary when considering questions that focus on station-level variables. 

The following code chunk selects station name and line, and then uses `distinct()` from `dplyr` to select only unique/distinct rows from the data frame. As a result, the number of rows in this dataset will be equal to the number of unique stations. We see that there are 465 unique stations.

```{r}
subway_data %>%
  select(station_name, line) %>%
  distinct
```

In the next code chunk we now filter according to ADA compliance as an initial step. This produces a dataframe where the number of rows is equal to the number of ADA compliant stations. We see that there are 84 ADA compliant stations.

```{r}
subway_data %>% 
  filter(ada == TRUE) %>% 
  select(station_name, line) %>% 
  distinct
```

To compute the proportion of station entrances / exits without vending that allow entrance, we first exclude station entrances that do not allow vending using `filter()`. Then, we focus on the `entry` variable using `pull()`. Since `entry` is a logical variable, we're able to get to the proportion we want by taking the mean of `entry` (since R will treat logical as numeric when taking the mean). We see that the proportion of station entrances / exits without vending that allow entrance is 0.3770492 or about 37.70% of station entrances / exits.

```{r}
subway_data %>%
  filter(vending == "NO") %>% 
  pull(entry) %>% 
  mean
```

For the last part of Question 1, we need to write a code chunk to identify stations that serve the A train, and to assess how many of these are ADA compliant. First, we tidy the data by converting `route` from wide to long format. Next, we can use tools from previous parts of the question (filtering to focus on the A train, and on ADA compliance; selecting and using `distinct` to obtain dataframes with the required stations in rows). We see that 60 stations serve the A train, and 17 of these stations are ADA compliant.

```{r}
subway_data %>%
    pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") %>% 
  filter(route == "A") %>% 
  select(station_name, line) %>% 
  distinct

subway_data %>%
    pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") %>% 
  filter(route == "A", ada == TRUE) %>% 
  select(station_name, line) %>% 
  distinct
```


# Problem 2
*This problem uses the Mr. Trash Wheel dataset, available as an Excel file on the course website.Read and clean the Mr. Trash Wheel sheet:*

* specify the sheet in the Excel file and to omit non-data entries (rows with notes / figures; columns containing notes) using arguments in read_excel
* use reasonable variable names
* omit rows that do not include dumpster-specific data
* round the number of sports balls to the nearest integer and converts the result to an integer variable (using as.integer)

```{r, "loading and cleaning mr_trash_data"}
mr_trash_data =
  read_excel("./data/Trash-Wheel-Collection-Data.xlsx",
             sheet = "Mr. Trash Wheel", range = "A2:N549") %>%
  janitor::clean_names() %>%
  rename(weight = weight_tons, volume = volume_cubic_yards) %>%
  drop_na(dumpster) %>%
  mutate(sports_balls = as.integer(round(sports_balls, digits = 0)))
```


*Use a similar process to import, clean, and organize the data for Professor Trash Wheel, and combine this with the Mr. Trash Wheel dataset to produce a single tidy dataset. To keep track of which Trash Wheel is which, you may need to add an additional variable to both datasets before combining.*

```{r, "loading and cleaning prof_trash_data"}
prof_trash_data =
  read_excel("./data/Trash-Wheel-Collection-Data.xlsx",
             sheet = "Professor Trash Wheel", range = "A2:M96") %>%
  janitor::clean_names() %>%
  rename(weight = weight_tons, volume = volume_cubic_yards) %>%
  drop_na(dumpster)
```

In the next code chunk, we combine the data for Professor Trash Wheel and the Mr. Trash Wheel datasets to produce a single tidy dataset while keeping track of which Trash Wheel is which.

```{r, "combining data for mr. and prof trash wheel"}
mr_trash_data =
  mutate(mr_trash_data, mr_prof = 0, year = as.numeric(year))

prof_trash_data =
  mutate(prof_trash_data, mr_prof = 1)

both_trash_data = 
  full_join(mr_trash_data, prof_trash_data)

both_trash_data
```

*Write a paragraph about these data; you are encouraged to use inline R. Be sure to note the number of observations in the resulting dataset, and give examples of key variables. For available data, what was the total weight of trash collected by Professor Trash Wheel? What was the total number of sports balls collected by Mr. Trash Wheel in 2020?*

### Describing the Joined Dataframe for Mr. Trash Wheel and Professor Trash Wheel Data

The joined dataset for Mr. Trash Wheel and Professor Trash Wheels Data contains a total of **`r nrow(both_trash_data)` rows/observations** and **`r ncol(both_trash_data)` columns/variables** (including an extra newly created variable from the previous code chunk, which we used to keep track of which original dataset an observation came from).

Key variables for the both_trash_data joined dataframe include: **`r colnames(both_trash_data)`**. All of these variables with the exception of `mr_prof` were from the original datasets; `mr_prof` is a variable to keep track of which dataset the observation is from (Mr. Trash Wheel = 0, Professor Trash Wheel = 1). All of these variables were numeric types (specifically, doubles) with the exception of:

* **1 integer variable:** `Sports Balls` (only in the Mr. Trash Wheel dataset originally)
* **1 character variable:** `Month`
* **1 date/posix variable** `Date`

**For available data, the total weight of trash collected by Professor Trash Wheel was `r both_trash_data %>% filter(mr_prof == 1) %>% pull(weight) %>% sum()` tons**.

**The total number of sports balls collected by Mr. Trash Wheel in 2020 was `r both_trash_data %>% filter(mr_prof == 0, year == 2020) %>% pull(sports_balls) %>% sum()` sports balls**.


# Problem 3

*This problem uses the FiveThirtyEight data; these data were gathered to create the interactive graphic on this page. In particular, we’ll use the data in pols-month.csv, unemployment.csv, and snp.csv. Our goal is to merge these into a single data frame using year and month as keys across datasets.*

*First, clean the data in pols-month.csv. Use separate() to break up the variable mon into integer variables year, month, and day; replace month number with month name; create a president variable taking values gop and dem, and remove prez_dem and prez_gop; and remove the day variable.*
```{r}
pols_data =
  read_csv(file = "./data/fivethirtyeight_datasets/pols-month.csv") %>%
  janitor::clean_names() %>%
  mutate(mon = lubridate::ymd(mon)) %>%
  separate(mon, into = c("year", "month", "day"), sep = "-") %>%
  mutate(month = lubridate::month(as.numeric(month), label = TRUE, abbr = FALSE)) %>%
  pivot_longer(
    cols = c(prez_gop, prez_dem),
    names_to = "president",
    names_prefix = "prez_",
    values_to = "party") %>%
  filter(party != 0) %>%
  select(-party, -day)

pols_data
```


*Second, clean the data in snp.csv using a similar process to the above. For consistency across datasets, arrange according to year and month, and organize so that year and month are the leading columns.*

```{r}
snp_data =
  read_csv(file = "./data/fivethirtyeight_datasets/snp.csv") %>%
  janitor::clean_names() %>%
  separate(date, into = c("month", "day", "year"), sep = "/") %>%
  mutate(year = case_when(
    year < 40 ~ paste0("20", year), 
    year >= 40 ~ paste0("19", year)
  )) %>%
  mutate(month = lubridate::month(as.numeric(month), label = TRUE, abbr = FALSE)) %>%
  select(year, everything(), -day) %>%
  arrange(year, month)


snp_data
```


*Third, tidy the unemployment data so that it can be merged with the previous datasets. This process will involve switching from “wide” to “long” format; ensuring that key variables have the same name; and ensuring that key variables take the same values.*

```{r}
unemploy_data =
  read_csv(file = "./data/fivethirtyeight_datasets/unemployment.csv") %>%
  janitor::clean_names() %>%
  pivot_longer(jan:dec,
    names_to = "month",
    values_to = "unemployment") %>%
  mutate(year = as.character(year)) %>%
  mutate(month = recode(month, "jan" = "1", "feb" = "2", "mar" = "3", "apr" = "4", "may" = "5", "jun" = "6", "jul" = "7", "aug" = "8", "sep" = "9", "oct" = "10", "nov" = "11", "dec" = "12")) %>%
  mutate(month = lubridate::month(as.numeric(month), label = TRUE, abbr = FALSE)) %>%
  arrange(year, month)

unemploy_data
```


*Join the datasets by merging snp into pols, and merging unemployment into the result.*

```{r}
pols_snp_df =
  full_join(pols_data, snp_data)

pols_snp_unemploy_df =
  full_join(pols_snp_df, unemploy_data)

```

*Write a short paragraph about these datasets. Explain briefly what each dataset contained, and describe the resulting dataset (e.g. give the dimension, range of years, and names of key variables).*

### Describing the Pols-Month (pols_data) Dataset

The original Pols-Month dataset contains 822 observations/rows of 9 variables/columns related to the number of national politicians who are democratic or republican at any given time.

To clean/tidy the dataset, we separated out the day, month, and year into new variables from the original `mon` (date) variable; we only kept the new `month` and `year` variables. We also combined the `prez_gop` and `prez_dom` variables into a new variable `president` which contains values on what political party the president at that date was associated with. After cleaning/tidying, the dataset contains a total of **`r nrow(pols_data)` rows/observations** and **`r ncol(pols_data)` columns/variables**. Key variables in the cleaned/tidied `pols_data` dataset include: **`r colnames(pols_data)`**.


### Describing the SNP (snp_data) Dataset

The original SNP dataset contains 787 observations/rows of 2 variables/columns related to Standard & Poor’s stock market index (S&P), often used as a representative measure of stock market as a whole. The original, untouched dataset contains two variables: `date` (the date of the observation including year, month, and day) and `close` (snp closing values).

To clean/tidy the dataset, we separated out the day, month, and year into new variables from the original `date` variable; we only kept the new `month` and `year` variables, and had to update the values of the `year` variable to contain the 4-digit year (the original data only included 2-digit year abbreviations for dates which spanned from 1950-2015). After cleaning/tidying, the `snp_data` dataset contains a total of **`r nrow(snp_data)` rows/observations** and **`r ncol(snp_data)` columns/variables**. Key variables in the cleaned/tidied `snp_data` dataset include: **`r colnames(snp_data)`**.



### Describing the Unemployment (unemploy_data) Dataset

The original Unemployment dataset contains 68 observations/rows of 13 variables/columns; the 13 variables included one variable for `Year` of the observation, and 12 variables for each month's unemployment rate.

To clean/tidy the dataset, we used `pivot_longer` to create a new variable  called `month` which contained the month of each observation, and another new variable called `unemployment` which stores the unemployment rate data for each observation. After cleaning/tidying, the dataset contains a total of **`r nrow(unemploy_data)` rows/observations** and **`r ncol(unemploy_data)` columns/variables**. Key variables in the cleaned/tidied `unemploy_data` dataset include: **`r colnames(unemploy_data)`**.


### Describing the Merged Dataset with Pols-Month/SNP/Unemployment Data (pols_snp_unemploy_df)

We merged the three datasets by the `Year` and `Month` variables, first left joining `pols_data` and `snp_data`, and then left-joining that with `unemploy_data`.

The resulting merged dataset `pols_snp_unemploy_df` contains a total of **`r nrow(pols_snp_unemploy_df)` rows/observations** and **`r ncol(pols_snp_unemploy_df)` columns/variables**. Key variables in the cleaned/tidied `pols_snp_unemploy_df` dataset include: **`r colnames(pols_snp_unemploy_df)`**.



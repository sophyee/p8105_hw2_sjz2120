p8105_hw2_sjz2120
================
2022-09-28

# Setup

``` r
library(tidyverse)
```

    ## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──
    ## ✔ ggplot2 3.3.6      ✔ purrr   0.3.4 
    ## ✔ tibble  3.1.8      ✔ dplyr   1.0.10
    ## ✔ tidyr   1.2.0      ✔ stringr 1.4.1 
    ## ✔ readr   2.1.2      ✔ forcats 0.5.2 
    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()

``` r
library(readxl)
```

# Problem 1

## Data Import and Cleaning

Below we import and clean data from
`NYC_Transit_Subway_Entrance_And_Exit_Data.csv`. First we use import the
data using `read_csv()`, update variable names, and select the columns
that will be used in later in this homework. We update the variable
`entry` from `yes` / `no` to a logical variable. As part of the data
import, we specify that `Route` columns 8-11 should be character for
consistency with columns 1-7.

``` r
subway_data =
  read_csv(file = "./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
           col_types = cols(Route8 = "c", Route9 = "c", Route10 = "c", Route11 = "c")) %>% 
  janitor::clean_names() %>% 
  select(line, station_name, station_latitude, station_longitude, starts_with("route"),
         entry, exit_only, vending, entrance_type, ada) %>% 
  mutate(entry = recode(entry, "YES" = TRUE, "NO" = FALSE ))
```

We still need to “tidy” up subway_data a bit. For instance, route number
and route should be variables, so we need to convert all of the `route`
variables from wide to long format. This will be useful when focusing on
specific routes, but may not be necessary when considering questions
that focus on station-level variables.

The following code chunk selects station name and line, and then uses
`distinct()` from `dplyr` to select only unique/distinct rows from the
data frame. As a result, the number of rows in this dataset will be
equal to the number of unique stations. We see that there are 465 unique
stations.

``` r
subway_data %>%
  select(station_name, line) %>%
  distinct
```

    ## # A tibble: 465 × 2
    ##    station_name             line    
    ##    <chr>                    <chr>   
    ##  1 25th St                  4 Avenue
    ##  2 36th St                  4 Avenue
    ##  3 45th St                  4 Avenue
    ##  4 53rd St                  4 Avenue
    ##  5 59th St                  4 Avenue
    ##  6 77th St                  4 Avenue
    ##  7 86th St                  4 Avenue
    ##  8 95th St                  4 Avenue
    ##  9 9th St                   4 Avenue
    ## 10 Atlantic Av-Barclays Ctr 4 Avenue
    ## # … with 455 more rows

In the next code chunk we now filter according to ADA compliance as an
initial step. This produces a dataframe where the number of rows is
equal to the number of ADA compliant stations. We see that there are 84
ADA compliant stations.

``` r
subway_data %>% 
  filter(ada == TRUE) %>% 
  select(station_name, line) %>% 
  distinct
```

    ## # A tibble: 84 × 2
    ##    station_name                   line           
    ##    <chr>                          <chr>          
    ##  1 Atlantic Av-Barclays Ctr       4 Avenue       
    ##  2 DeKalb Av                      4 Avenue       
    ##  3 Pacific St                     4 Avenue       
    ##  4 Grand Central                  42nd St Shuttle
    ##  5 34th St                        6 Avenue       
    ##  6 47-50th Sts Rockefeller Center 6 Avenue       
    ##  7 Church Av                      6 Avenue       
    ##  8 21st St                        63rd Street    
    ##  9 Lexington Av                   63rd Street    
    ## 10 Roosevelt Island               63rd Street    
    ## # … with 74 more rows

To compute the proportion of station entrances / exits without vending
that allow entrance, we first exclude station entrances that do not
allow vending using `filter()`. Then, we focus on the `entry` variable
using `pull()`. Since `entry` is a logical variable, we’re able to get
to the proportion we want by taking the mean of `entry` (since R will
treat logical as numeric when taking the mean). We see that the
proportion of station entrances / exits without vending that allow
entrance is 0.3770492 or about 37.70% of station entrances / exits.

``` r
subway_data %>%
  filter(vending == "NO") %>% 
  pull(entry) %>% 
  mean
```

    ## [1] 0.3770492

For the last part of Question 1, we need to write a code chunk to
identify stations that serve the A train, and to assess how many of
these are ADA compliant. First, we tidy the data by converting `route`
from wide to long format. Next, we can use tools from previous parts of
the question (filtering to focus on the A train, and on ADA compliance;
selecting and using `distinct` to obtain dataframes with the required
stations in rows). We see that 60 stations serve the A train, and 17 of
these stations are ADA compliant.

``` r
subway_data %>%
    pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") %>% 
  filter(route == "A") %>% 
  select(station_name, line) %>% 
  distinct
```

    ## # A tibble: 60 × 2
    ##    station_name                  line           
    ##    <chr>                         <chr>          
    ##  1 Times Square                  42nd St Shuttle
    ##  2 125th St                      8 Avenue       
    ##  3 145th St                      8 Avenue       
    ##  4 14th St                       8 Avenue       
    ##  5 168th St - Washington Heights 8 Avenue       
    ##  6 175th St                      8 Avenue       
    ##  7 181st St                      8 Avenue       
    ##  8 190th St                      8 Avenue       
    ##  9 34th St                       8 Avenue       
    ## 10 42nd St                       8 Avenue       
    ## # … with 50 more rows

``` r
subway_data %>%
    pivot_longer(
    route1:route11,
    names_to = "route_num",
    values_to = "route") %>% 
  filter(route == "A", ada == TRUE) %>% 
  select(station_name, line) %>% 
  distinct
```

    ## # A tibble: 17 × 2
    ##    station_name                  line            
    ##    <chr>                         <chr>           
    ##  1 14th St                       8 Avenue        
    ##  2 168th St - Washington Heights 8 Avenue        
    ##  3 175th St                      8 Avenue        
    ##  4 34th St                       8 Avenue        
    ##  5 42nd St                       8 Avenue        
    ##  6 59th St                       8 Avenue        
    ##  7 Inwood - 207th St             8 Avenue        
    ##  8 West 4th St                   8 Avenue        
    ##  9 World Trade Center            8 Avenue        
    ## 10 Times Square-42nd St          Broadway        
    ## 11 59th St-Columbus Circle       Broadway-7th Ave
    ## 12 Times Square                  Broadway-7th Ave
    ## 13 8th Av                        Canarsie        
    ## 14 Franklin Av                   Franklin        
    ## 15 Euclid Av                     Fulton          
    ## 16 Franklin Av                   Fulton          
    ## 17 Howard Beach                  Rockaway

# Problem 2

*This problem uses the Mr. Trash Wheel dataset, available as an Excel
file on the course website.Read and clean the Mr. Trash Wheel sheet:*

-   specify the sheet in the Excel file and to omit non-data entries
    (rows with notes / figures; columns containing notes) using
    arguments in read_excel
-   use reasonable variable names
-   omit rows that do not include dumpster-specific data
-   round the number of sports balls to the nearest integer and converts
    the result to an integer variable (using as.integer)

``` r
mr_trash_data =
  read_excel("./data/Trash-Wheel-Collection-Data.xlsx",
             sheet = "Mr. Trash Wheel", range = "A2:N549") %>%
  janitor::clean_names() %>%
  rename(weight = weight_tons, volume = volume_cubic_yards) %>%
  drop_na(dumpster) %>%
  mutate(sports_balls = as.integer(round(sports_balls, digits = 0)))
```

*Use a similar process to import, clean, and organize the data for
Professor Trash Wheel, and combine this with the Mr. Trash Wheel dataset
to produce a single tidy dataset. To keep track of which Trash Wheel is
which, you may need to add an additional variable to both datasets
before combining.*

``` r
prof_trash_data =
  read_excel("./data/Trash-Wheel-Collection-Data.xlsx",
             sheet = "Professor Trash Wheel", range = "A2:M96") %>%
  janitor::clean_names() %>%
  rename(weight = weight_tons, volume = volume_cubic_yards) %>%
  drop_na(dumpster)
```

In the next code chunk, we combine the data for Professor Trash Wheel
and the Mr. Trash Wheel datasets to produce a single tidy dataset while
keeping track of which Trash Wheel is which.

``` r
mr_trash_data =
  mutate(mr_trash_data, mr_prof = 0, year = as.numeric(year))

prof_trash_data =
  mutate(prof_trash_data, mr_prof = 1)

both_trash_data = 
  full_join(mr_trash_data, prof_trash_data)
```

    ## Joining, by = c("dumpster", "month", "year", "date", "weight", "volume",
    ## "plastic_bottles", "polystyrene", "cigarette_butts", "glass_bottles",
    ## "grocery_bags", "chip_bags", "homes_powered", "mr_prof")

``` r
both_trash_data
```

    ## # A tibble: 641 × 15
    ##    dumps…¹ month  year date                weight volume plast…² polys…³ cigar…⁴
    ##      <dbl> <chr> <dbl> <dttm>               <dbl>  <dbl>   <dbl>   <dbl>   <dbl>
    ##  1       1 May    2014 2014-05-16 00:00:00   4.31     18    1450    1820  126000
    ##  2       2 May    2014 2014-05-16 00:00:00   2.74     13    1120    1030   91000
    ##  3       3 May    2014 2014-05-16 00:00:00   3.45     15    2450    3100  105000
    ##  4       4 May    2014 2014-05-17 00:00:00   3.1      15    2380    2730  100000
    ##  5       5 May    2014 2014-05-17 00:00:00   4.06     18     980     870  120000
    ##  6       6 May    2014 2014-05-20 00:00:00   2.71     13    1430    2140   90000
    ##  7       7 May    2014 2014-05-21 00:00:00   1.91      8     910    1090   56000
    ##  8       8 May    2014 2014-05-28 00:00:00   3.7      16    3580    4310  112000
    ##  9       9 June   2014 2014-06-05 00:00:00   2.52     14    2400    2790   98000
    ## 10      10 June   2014 2014-06-11 00:00:00   3.76     18    1340    1730  130000
    ## # … with 631 more rows, 6 more variables: glass_bottles <dbl>,
    ## #   grocery_bags <dbl>, chip_bags <dbl>, sports_balls <int>,
    ## #   homes_powered <dbl>, mr_prof <dbl>, and abbreviated variable names
    ## #   ¹​dumpster, ²​plastic_bottles, ³​polystyrene, ⁴​cigarette_butts

*Write a paragraph about these data; you are encouraged to use inline R.
Be sure to note the number of observations in the resulting dataset, and
give examples of key variables. For available data, what was the total
weight of trash collected by Professor Trash Wheel? What was the total
number of sports balls collected by Mr. Trash Wheel in 2020?*

### Describing the Joined Dataframe for Mr. Trash Wheel and Professor Trash Wheel Data

The joined dataset for Mr. Trash Wheel and Professor Trash Wheels Data
contains a total of **641 rows/observations** and **15
columns/variables** (including an extra newly created variable from the
previous code chunk, which we used to keep track of which original
dataset an observation came from).

Key variables for the both_trash_data joined dataframe include:
**dumpster, month, year, date, weight, volume, plastic_bottles,
polystyrene, cigarette_butts, glass_bottles, grocery_bags, chip_bags,
sports_balls, homes_powered, mr_prof**. All of these variables with the
exception of `mr_prof` were from the original datasets; `mr_prof` is a
variable to keep track of which dataset the observation is from
(Mr. Trash Wheel = 0, Professor Trash Wheel = 1). All of these variables
were numeric types (specifically, doubles) with the exception of:

-   **1 integer variable:** `Sports Balls` (only in the Mr. Trash Wheel
    dataset originally)
-   **1 character variable:** `Month`
-   **1 date/posix variable** `Date`

**For available data, the total weight of trash collected by Professor
Trash Wheel was 190.12 tons**.

**The total number of sports balls collected by Mr. Trash Wheel in 2020
was 856 sports balls**.

# Problem 3

*This problem uses the FiveThirtyEight data; these data were gathered to
create the interactive graphic on this page. In particular, we’ll use
the data in pols-month.csv, unemployment.csv, and snp.csv. Our goal is
to merge these into a single data frame using year and month as keys
across datasets.*

*First, clean the data in pols-month.csv. Use separate() to break up the
variable mon into integer variables year, month, and day; replace month
number with month name; create a president variable taking values gop
and dem, and remove prez_dem and prez_gop; and remove the day variable.*

``` r
pols_data =
  read_csv(file = "./data/fivethirtyeight_datasets/pols-month.csv") %>%
  janitor::clean_names() %>%
  mutate(mon = lubridate::ymd(mon)) %>%
  separate(mon, into = c("year", "month", "day"), sep = "-") %>%
  mutate(month = lubridate::month(as.numeric(month), label = TRUE, abbr = FALSE)) %>%
  pivot_longer(
    cols = c(prez_gop, prez_dem),
    names_to = "president",
    names_prefix = "prez_",
    values_to = "party") %>%
  filter(party != 0) %>%
  select(-party, -day)
```

    ## Rows: 822 Columns: 9
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl  (8): prez_gop, gov_gop, sen_gop, rep_gop, prez_dem, gov_dem, sen_dem, r...
    ## date (1): mon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
pols_data
```

    ## # A tibble: 822 × 9
    ##    year  month     gov_gop sen_gop rep_gop gov_dem sen_dem rep_dem president
    ##    <chr> <ord>       <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl> <chr>    
    ##  1 1947  January        23      51     253      23      45     198 dem      
    ##  2 1947  February       23      51     253      23      45     198 dem      
    ##  3 1947  March          23      51     253      23      45     198 dem      
    ##  4 1947  April          23      51     253      23      45     198 dem      
    ##  5 1947  May            23      51     253      23      45     198 dem      
    ##  6 1947  June           23      51     253      23      45     198 dem      
    ##  7 1947  July           23      51     253      23      45     198 dem      
    ##  8 1947  August         23      51     253      23      45     198 dem      
    ##  9 1947  September      23      51     253      23      45     198 dem      
    ## 10 1947  October        23      51     253      23      45     198 dem      
    ## # … with 812 more rows

*Second, clean the data in snp.csv using a similar process to the above.
For consistency across datasets, arrange according to year and month,
and organize so that year and month are the leading columns.*

``` r
snp_data =
  read_csv(file = "./data/fivethirtyeight_datasets/snp.csv") %>%
  janitor::clean_names() %>%
  separate(date, into = c("month", "day", "year"), sep = "/") %>%
  mutate(year = case_when(
    year < 40 ~ paste0("20", year), 
    year >= 40 ~ paste0("19", year)
  )) %>%
  mutate(month = lubridate::month(as.numeric(month), label = TRUE, abbr = FALSE)) %>%
  select(year, everything(), -day) %>%
  arrange(year, month)
```

    ## Rows: 787 Columns: 2
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (1): date
    ## dbl (1): close
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
snp_data
```

    ## # A tibble: 787 × 3
    ##    year  month     close
    ##    <chr> <ord>     <dbl>
    ##  1 1950  January    17.0
    ##  2 1950  February   17.2
    ##  3 1950  March      17.3
    ##  4 1950  April      18.0
    ##  5 1950  May        18.8
    ##  6 1950  June       17.7
    ##  7 1950  July       17.8
    ##  8 1950  August     18.4
    ##  9 1950  September  19.5
    ## 10 1950  October    19.5
    ## # … with 777 more rows

*Third, tidy the unemployment data so that it can be merged with the
previous datasets. This process will involve switching from “wide” to
“long” format; ensuring that key variables have the same name; and
ensuring that key variables take the same values.*

``` r
unemploy_data =
  read_csv(file = "./data/fivethirtyeight_datasets/unemployment.csv") %>%
  janitor::clean_names() %>%
  pivot_longer(jan:dec,
    names_to = "month",
    values_to = "unemployment") %>%
  mutate(year = as.character(year)) %>%
  mutate(month = recode(month, "jan" = "1", "feb" = "2", "mar" = "3", "apr" = "4", "may" = "5", "jun" = "6", "jul" = "7", "aug" = "8", "sep" = "9", "oct" = "10", "nov" = "11", "dec" = "12")) %>%
  mutate(month = lubridate::month(as.numeric(month), label = TRUE, abbr = FALSE)) %>%
  arrange(year, month)
```

    ## Rows: 68 Columns: 13
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (13): Year, Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

``` r
unemploy_data
```

    ## # A tibble: 816 × 3
    ##    year  month     unemployment
    ##    <chr> <ord>            <dbl>
    ##  1 1948  January            3.4
    ##  2 1948  February           3.8
    ##  3 1948  March              4  
    ##  4 1948  April              3.9
    ##  5 1948  May                3.5
    ##  6 1948  June               3.6
    ##  7 1948  July               3.6
    ##  8 1948  August             3.9
    ##  9 1948  September          3.8
    ## 10 1948  October            3.7
    ## # … with 806 more rows

*Join the datasets by merging snp into pols, and merging unemployment
into the result.*

``` r
merged_df =
  left_join(pols_data, snp_data) %>%
  left_join(., unemploy_data)
```

    ## Joining, by = c("year", "month")
    ## Joining, by = c("year", "month")

*Write a short paragraph about these datasets. Explain briefly what each
dataset contained, and describe the resulting dataset (e.g. give the
dimension, range of years, and names of key variables).*

### Describing the Pols-Month (pols_data) Dataset

The original Pols-Month dataset contains 822 observations/rows of 9
variables/columns related to the number of national politicians who are
democratic or republican at any given time.

To clean/tidy the dataset, we separated out the day, month, and year
into new variables from the original `mon` (date) variable; we only kept
the new `month` and `year` variables. We also combined the `prez_gop`
and `prez_dom` variables into a new variable `president` which contains
values on what political party the president at that date was associated
with. After cleaning/tidying, the dataset contains a total of **822
rows/observations** and **9 columns/variables**. Key variables in the
cleaned/tidied `pols_data` dataset include: **year, month, gov_gop,
sen_gop, rep_gop, gov_dem, sen_dem, rep_dem, president**.

### Describing the SNP (snp_data) Dataset

The original SNP dataset contains 787 observations/rows of 2
variables/columns related to Standard & Poor’s stock market index (S&P),
often used as a representative measure of stock market as a whole. The
original, untouched dataset contains two variables: `date` (the date of
the observation including year, month, and day) and `close` (snp closing
values).

To clean/tidy the dataset, we separated out the day, month, and year
into new variables from the original `date` variable; we only kept the
new `month` and `year` variables, and had to update the values of the
`year` variable to contain the 4-digit year (the original data only
included 2-digit year abbreviations for dates which spanned from
1950-2015). After cleaning/tidying, the `snp_data` dataset contains a
total of **787 rows/observations** and **3 columns/variables**. Key
variables in the cleaned/tidied `snp_data` dataset include: **year,
month, close**.

### Describing the Unemployment (unemploy_data) Dataset

The original Unemployment dataset contains 68 observations/rows of 13
variables/columns; the 13 variables included one variable for `Year` of
the observation, and 12 variables for each month’s unemployment rate.

To clean/tidy the dataset, we used `pivot_longer` to create a new
variable called `month` which contained the month of each observation,
and another new variable called `unemployment` which stores the
unemployment rate data for each observation. After cleaning/tidying, the
dataset contains a total of **816 rows/observations** and **3
columns/variables**. Key variables in the cleaned/tidied `unemploy_data`
dataset include: **year, month, unemployment**.

### Describing the Merged Dataset with Pols-Month/SNP/Unemployment Data (merged_df)

We merged the three datasets by the `Year` and `Month` variables, first
left-joining `pols_data` and `snp_data`, and then left-joining that with
`unemploy_data`.

The resulting merged dataset `merged_df` contains a total of **822
rows/observations** and **11 columns/variables**. Key variables in the
cleaned/tidied `merged_df` dataset include: **year, month, gov_gop,
sen_gop, rep_gop, gov_dem, sen_dem, rep_dem, president, close,
unemployment**.
